{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Song1.txt', 'r') as myfile:\n",
    "    Song1=myfile.read().replace('\\n','. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Song2.txt', 'r') as myfile:\n",
    "    Song2=myfile.read().replace('\\n','. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Song3.txt', 'r') as myfile:\n",
    "    Song3=myfile.read().replace('\\n','. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Song4.txt', 'r') as myfile:\n",
    "    Song4=myfile.read().replace('\\n','. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Song5.txt', 'r') as myfile:\n",
    "    Song5=myfile.read().replace('\\n','. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Song6.txt', 'r') as myfile:\n",
    "    Song6=myfile.read().replace('\\n','. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Song7.txt', 'r') as myfile:\n",
    "    Song7=myfile.read().replace('\\n','. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Song8.txt', 'r') as myfile:\n",
    "    Song8=myfile.read().replace('\\n','. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Song9.txt', 'r') as myfile:\n",
    "    Song9=myfile.read().replace('\\n','. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Song10.txt', 'r') as myfile:\n",
    "    Song10=myfile.read().replace('\\n','. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_songs=[Song1,Song2,Song3,Song4,Song5,Song6,Song7,Song8,Song9,Song10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_songs=[word_tokenize(d) for d in all_songs]\n",
    "all_tokens_set=set([item for sublist in tokenized_songs for item in sublist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.2825475642657122, 0.1050182346887792, 0.062025461161650655, 0.03179879752729464, 0.14437182627431316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13016739500414665, 0.1050182346887792, 0.0, 0.10537054046253023, 0.1050182346887792, 0.0, 0.0, 0.0, 0.0, 0.043566447135259, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08297698783753836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062025461161650655, 0.1050182346887792, 0.0, 0.062025461161650655, 0.2729862972610446, 0.16185158989142007, 0.14801100821590776, 0.060935740986297525, 0.0, 0.0, 0.1050182346887792, 0.12788214775944573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062025461161650655, 0.0, 0.0, 0.0, 0.0, 0.1050182346887792, 0.13016739500414665, 0.1050182346887792, 0.13016739500414665, 0.0, 0.062025461161650655, 0.0, 0.0, 0.0, 0.0, 0.048042438109183364, 0.0, 0.035989630249477364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062025461161650655, 0.0, 0.062025461161650655, 0.0, 0.062025461161650655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04139255332619497, 0.0, 0.0, 0.1050182346887792, 0.08297698783753836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0700836849604242, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062025461161650655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0700836849604242, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10284783765516069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08297698783753836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04900754570556401, 0.0, 0.0, 0.0, 0.0, 0.04900754570556401, 0.0, 0.0, 0.0, 0.0700836849604242, 0.0, 0.0, 0.0, 0.0, 0.0553267911017826, 0.0, 0.0, 0.13285708780937214, 0.0, 0.0, 0.07552828030617473, 0.028374637870108327, 0.0, 0.1050182346887792, 0.0, 0.1050182346887792, 0.0700836849604242, 0.0700836849604242, 0.04900754570556401, 0.062025461161650655, 0.0, 0.0, 0.0, 0.062025461161650655, 0.053840044378535484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11506693036287209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05954736372071687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0700836849604242, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062025461161650655, 0.0, 0.04900754570556401, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04314063185266156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1050182346887792, 0.0, 0.1050182346887792, 0.0, 0.1050182346887792, 0.0, 0.0, 0.04314063185266156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1050182346887792, 0.14001304109969223, 0.04900754570556401, 0.0, 0.0, 0.0, 0.0, 0.062025461161650655, 0.048042438109183364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03179879752729464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04900754570556401, 0.062025461161650655, 0.0, 0.062025461161650655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062025461161650655, 0.062025461161650655, 0.0, 0.0, 0.0, 0.062025461161650655, 0.0, 0.062025461161650655, 0.0, 0.062025461161650655, 0.0, 0.0, 0.0, 0.0, 0.1050182346887792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1050182346887792, 0.0, 0.0, 0.04900754570556401, 0.0, 0.0, 0.0, 0.0, 0.08297698783753836, 0.11694642996951272, 0.0, 0.062025461161650655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1050182346887792, 0.0, 0.0, 0.0, 0.062025461161650655, 0.1050182346887792, 0.0, 0.062025461161650655, 0.0, 0.0, 0.0, 0.0, 0.1050182346887792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1050182346887792, 0.0, 0.062025461161650655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062025461161650655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08297698783753836, 0.0, 0.12189900918237398, 0.0700836849604242, 0.0, 0.062025461161650655, 0.0, 0.057834628439467926, 0.062025461161650655, 0.0, 0.08297698783753836, 0.0, 0.0, 0.0700836849604242, 0.0, 0.0, 0.0, 0.0, 0.04900754570556401, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05795593687658891, 0.1050182346887792, 0.0, 0.1050182346887792, 0.22999352373391604, 0.0, 0.0, 0.0, 0.0, 0.04900754570556401, 0.0, 0.0, 0.0, 0.0, 0.04139255332619497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03179879752729464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1050182346887792, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1050182346887792, 0.0, 0.0, 0.0, 0.062025461161650655, 0.0, 0.0, 0.0, 0.08297698783753836, 0.0, 0.062025461161650655, 0.1050182346887792, 0.07234542661791883, 0.0, 0.08686692106970281, 0.0, 0.0, 0.048042438109183364, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(all_songs)\n",
    "print(sklearn_representation.toarray()[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "        dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "        magnitude = math.sqrt(sum([val**2 for val in vector1])*sum([val**2 for val in vector2]))\n",
    "        if not magnitude:\n",
    "            return 0\n",
    "        return dot_product/magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.099581417492239782"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(sklearn_representation.toarray()[0],sklearn_representation.toarray()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0995814174922\n",
      "0.0858702802442\n",
      "0.100428585002\n",
      "0.104727161239\n",
      "0.072313411949\n",
      "0.12523036461\n",
      "0.118222601918\n",
      "0.0922385078299\n",
      "0.119492874157\n",
      "0.0995814174922\n",
      "1.0\n",
      "0.0890723340152\n",
      "0.0743927898855\n",
      "0.0585022879637\n",
      "0.0490018550571\n",
      "0.0542579853524\n",
      "0.0338845598612\n",
      "0.0739125260638\n",
      "0.052970672702\n",
      "0.0858702802442\n",
      "0.0890723340152\n",
      "1.0\n",
      "0.0827921099391\n",
      "0.127201761751\n",
      "0.085674959015\n",
      "0.0749545219387\n",
      "0.106078816441\n",
      "0.0726893228906\n",
      "0.129767948578\n",
      "0.100428585002\n",
      "0.0743927898855\n",
      "0.0827921099391\n",
      "1.0\n",
      "0.0931157193289\n",
      "0.108064260047\n",
      "0.068775827712\n",
      "0.112857066912\n",
      "0.0915933628643\n",
      "0.0754056059476\n",
      "0.104727161239\n",
      "0.0585022879637\n",
      "0.127201761751\n",
      "0.0931157193289\n",
      "1.0\n",
      "0.0709696843928\n",
      "0.0508884257085\n",
      "0.0941833981745\n",
      "0.0734110102704\n",
      "0.112527328936\n",
      "0.072313411949\n",
      "0.0490018550571\n",
      "0.085674959015\n",
      "0.108064260047\n",
      "0.0709696843928\n",
      "1.0\n",
      "0.117858150603\n",
      "0.0727211028913\n",
      "0.0487701979592\n",
      "0.0701325853727\n",
      "0.12523036461\n",
      "0.0542579853524\n",
      "0.0749545219387\n",
      "0.068775827712\n",
      "0.0508884257085\n",
      "0.117858150603\n",
      "1.0\n",
      "0.0648439859075\n",
      "0.0924618808305\n",
      "0.0907157203199\n",
      "0.118222601918\n",
      "0.0338845598612\n",
      "0.106078816441\n",
      "0.112857066912\n",
      "0.0941833981745\n",
      "0.0727211028913\n",
      "0.0648439859075\n",
      "1.0\n",
      "0.075339717894\n",
      "0.0870236263453\n",
      "0.0922385078299\n",
      "0.0739125260638\n",
      "0.0726893228906\n",
      "0.0915933628643\n",
      "0.0734110102704\n",
      "0.0487701979592\n",
      "0.0924618808305\n",
      "0.075339717894\n",
      "1.0\n",
      "0.0740437185571\n",
      "0.119492874157\n",
      "0.052970672702\n",
      "0.129767948578\n",
      "0.0754056059476\n",
      "0.112527328936\n",
      "0.0701325853727\n",
      "0.0907157203199\n",
      "0.0870236263453\n",
      "0.0740437185571\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for doc1 in range(0,10):\n",
    "    for doc2 in range(0,10):\n",
    "        print (cosine_similarity(sklearn_representation.toarray()[doc1],sklearn_representation.toarray()[doc2]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recompute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', 'piec', 'doorstep', 'swing', 'feel', 'they', 'money', 'lover', 'way', 'downtown', 'joint', 'rock-e-fellow', 'got', 'have', 'oh', \"n't\", 'sing', 'alway', 'all', 'better', 'fli', 'need', 'who', 'doe', 'skinni', 'from', 'what', 'am', 'like', 'in', 'whatev', 'than', 'old', 'shut', 'float', 'goodi', 'here..', 'then', 'broke', 'nobodi', 'two', 'booti', 'slow', 'plenti', 'away', 'black', 'right', 'yeah', 'should', 'taught', 'd', 'elev', 'get', 'drive', 'boy', 'start', 'bass', 'may', 'blue', 'key', 'music', 'lie', 'just', 'tell', 'never', 'bring', 'them', 'burn', 'pine', 'rover', 'heard', 'control', 'skin', 'dress', 'dream', 'know', 'suppos', 'now', 'caught', 'ay', 'bitch', 'an', 'sigh', 'weepi', 'size', 'smile', 'shade', 'grab', 'phrase', 'mate', 'want', 'swell', 'lucki', 'tune', 'love', 'learn', 'ba-ba-bo', 'out', 'life', 'bodi', 'list', \"'m\", 'well', 'give', 'pillow', 'thing', 'play', 'creep', 'direct', 'still', 'can', 'then', 'wait', 'whi', 'few', 'until', 'littl', 'haunt', 'worri', 'theme', 'We', 'fate', \"'bout\", 'side', 'kind', 'will', 'singin', 'down', 'ai', 'cent', 'may', 'have', 'satisfi', 'Oh', 'again', 'me', 'everi', 'spell', 'street', 'ca', 'high', 'spend', 'long', 'afreaid', 'coat', 'keep', 'chang', 'magazin', 'our', 'show', 'willow', 'fire', 'hallelujah', 'sway', 'In', 'zay-zoo-za-ze-zo-zay', 'tea', 'real', 'think', 'hooray', 'doll', 'turtl', \"'ve\", 'becaus', 'when', 'that', 'sip', 'If', 'bottom', 'if', 'ici', 'hey', 't', 'someth', 'momma', 'happi', 'on', \"'\", 'guess', 'play', 'need', 'dynamit', 'name', \"'s\", 'aflam', 'wa', 'make', 'lip', 'place', 'someon', 'angel', 'I', 'He', 'groov', 'diamond', 'off', 'run', 'bumpi', 'one', 'song', 'a', '.', 'junk', 'enough', 'back', 'she', 'no', 'here', 'whisper', 'sunni', 'meet', 'wan', 'hey', 'him', 'day', 'act', 'gee', 'hurt', 'magic', 'all', 'but', 'heel', 'darn', 'becaus', 'each', 'find', 'the', 'feel..', 'dust', '?', 'class', 'cross', 'sell', 'were', 'rais', 'ya', 'rich', 'witchcraft', 'kiss', 'up', 'inch', 'clear', 'that', 'scheme', 'along', 'not', 'wear', 'woolworth', 'nobodi', 'Ca', 'na', 'tingl', 'you', 'ba-be-oh', 'walk', 'those', 'permiss', 'my', 'see', 'ba-be-bo-ba-bay', 'true', 'worri', 'you', 'realli', 'she', 'ever', 'it', 'it', 'had', 'bit', '’', 'get', 'like', 'call', 'scheme', '!', 'tide', 'when', 'barbi', 'as', 'like', 'No', 'night', 'time', 'your', 'such', \"'ll\", \"'re\", 'face', 'ba-be-do', 'ride', 'too', 'eye', 'beauti', 'set', 'know', 'into', 'afraid', 'some', 'bust', 'there', 'frame', 'dream', 'bill', 'So', 'memori', 'onli', 'for', 'desir', 'feet', 'bracelet', 'gave', 'babi', 'boom', 'mother', 'coo', 'wo', 'know', 'trebl', 'woodland', 'been', 'go', 'abov', 'shake', 'mani', 'meant', 'look', 'cu', 'chase', 'special', 'till', 'take', 'crazi', 'who', \"'caus\", 'happi', 'fat', 'could', 'or', 'knife', 'step', 'barrel', 'weirdo..', 'bike', 'hold', 'run', 'farewel', 'lullabi', 'hear', 'ahead', 'look', 'how', 'cri', 'feather', 'scheme', 'say', 'ain', 'weav', 'pray', 'romanc', 'goodi', 'everi', 'babi', 'there', 'sweet', 'he', 'while', 'rascal', 'sure', 'danc', 'what', 'leav', 'is', 'and', 'come', 'rover', 'spin', 'Go', 'hell', 'insid', 'babe', 'foolish', 'work', 'leaf', 'danc', 'put', 'chanc', 'stay', 'eye', 'perfect', 'spine', 'tight', 'feel', 'do', 'just', 'On', 'do', 'exactli', 'pitter-pat', 'met', 'care', 'life', 'so', 'don', 'parad', 'move', 'over', 'blue', 'notic', 'befor', 'we', 'round', 'down', 'soul', 'beam', 'heart', 'three', 'but', 'hope', 'under', 'shit', 'about', 'use', 'with', 'make', \"'bout\", 'creat', 'more', 'world', 'weirdo', 'alway', 'dove', 'told', 'birdland', 'i', 'aflam', 'mine', 'word', 'from', 'road', ',', 'gold', 'stop', \"'em\", 'awak', 'exactli', 'silicon', 'whi', 'kiss', 'thing', 'come', 'lullabi', 'belong', 'same', 'matter', 'sky', 'are', 'your', 'top', 'care', 'and', 'beauti', 'understand', \"'d\", 'other', 'could', 'low', 'around', 'hat', 'grand', 'ha', 'reveal', 'listen', 'the', 'never', 'at', 'love', 'with', 'finger', 'photoshop', 'anyth', 'fuckin', 'stick-figur', 'be', 'My', 'wish', 'to', 'pretti', 'goodby', 'last', 'dream']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "text_stemmed=[porter.stem(t) for t in all_tokens_set]\n",
    "print(text_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize1=word_tokenize(Song1)\n",
    "Remove1=[w for w in tokenize1 if w not in stopwords.words('english')]\n",
    "result1 = ' '.join(Remove1)\n",
    "\n",
    "tokenize2=word_tokenize(Song2)\n",
    "Remove2=[w for w in tokenize2 if w not in stopwords.words('english')]\n",
    "result2 = ' '.join(Remove2)\n",
    "\n",
    "tokenize3=word_tokenize(Song3)\n",
    "Remove3=[w for w in tokenize3 if w not in stopwords.words('english')]\n",
    "result3 = ' '.join(Remove3)\n",
    "\n",
    "tokenize4=word_tokenize(Song4)\n",
    "Remove4=[w for w in tokenize4 if w not in stopwords.words('english')]\n",
    "result4 = ' '.join(Remove4)\n",
    "\n",
    "tokenize5=word_tokenize(Song5)\n",
    "Remove5=[w for w in tokenize5 if w not in stopwords.words('english')]\n",
    "result5 = ' '.join(Remove5)\n",
    "\n",
    "tokenize6=word_tokenize(Song6)\n",
    "Remove6=[w for w in tokenize6 if w not in stopwords.words('english')]\n",
    "result6 = ' '.join(Remove6)\n",
    "\n",
    "tokenize7=word_tokenize(Song7)\n",
    "Remove7=[w for w in tokenize7 if w not in stopwords.words('english')]\n",
    "result7 = ' '.join(Remove7)\n",
    "\n",
    "tokenize8=word_tokenize(Song8)\n",
    "Remove8=[w for w in tokenize8 if w not in stopwords.words('english')]\n",
    "result8 = ' '.join(Remove8)\n",
    "\n",
    "tokenize9=word_tokenize(Song9)\n",
    "Remove9=[w for w in tokenize9 if w not in stopwords.words('english')]\n",
    "result9 = ' '.join(Remove9)\n",
    "\n",
    "tokenize10=word_tokenize(Song10)\n",
    "Remove10=[w for w in tokenize10 if w not in stopwords.words('english')]\n",
    "result10 = ' '.join(Remove10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_no_stopwords=[result1,result2,result3,result4,result5,result6,result7,result8,result9,result10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.31839223255349586, 0.11834110227877498, 0.0, 0.06989416138036982, 0.0, 0.14971165972749856, 0.07519930130600108, 0.07492205040498014, 0.0, 0.11537150097259743, 0.10352789337214191, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14668074597899614, 0.11834110227877498, 0.04055536190405758, 0.11834110227877498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.055224761642213696, 0.11834110227877498, 0.0, 0.3277252972589398, 0.0, 0.0, 0.11834110227877498, 0.18238447456372436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06989416138036982, 0.0, 0.0, 0.0, 0.0, 0.11834110227877498, 0.14668074597899614, 0.11834110227877498, 0.14668074597899614, 0.0, 0.06989416138036982, 0.0, 0.0, 0.0, 0.0, 0.07897467096598572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06989416138036982, 0.0, 0.06989416138036982, 0.0, 0.06989416138036982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11834110227877498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07897467096598572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06989416138036982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06989416138036982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08510998086323776, 0.0, 0.0, 0.0, 0.09350364947160915, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.055224761642213696, 0.0, 0.07897467096598572, 0.0, 0.0, 0.0, 0.09383855631341892, 0.0, 0.046643712887303616, 0.0, 0.11834110227877498, 0.0, 0.0, 0.06989416138036982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12966459982241793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06710168649652329, 0.0, 0.0, 0.0, 0.0, 0.07897467096598572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06989416138036982, 0.0, 0.055224761642213696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11834110227877498, 0.0, 0.0, 0.09350364947160915, 0.0, 0.0, 0.1000367541131669, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07897467096598572, 0.046643712887303616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09350364947160915, 0.0, 0.06989416138036982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06989416138036982, 0.06989416138036982, 0.0, 0.0, 0.0, 0.06989416138036982, 0.0, 0.06989416138036982, 0.0, 0.06989416138036982, 0.0, 0.0, 0.0, 0.0, 0.11834110227877498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11834110227877498, 0.0, 0.0, 0.046643712887303616, 0.0, 0.0, 0.0, 0.09350364947160915, 0.07897467096598572, 0.06989416138036982, 0.0, 0.0, 0.0, 0.0, 0.11834110227877498, 0.0, 0.0, 0.0, 0.14668074597899614, 0.0, 0.06989416138036982, 0.0, 0.0, 0.0, 0.07897467096598572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11834110227877498, 0.0, 0.06989416138036982, 0.0, 0.0, 0.0, 0.06989416138036982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09350364947160915, 0.0, 0.0, 0.0, 0.11834110227877498, 0.0, 0.0, 0.0, 0.055224761642213696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11834110227877498, 0.11834110227877498, 0.25917105916235067, 0.0, 0.0, 0.0, 0.04055536190405758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04055536190405758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11834110227877498, 0.0, 0.0, 0.0, 0.06989416138036982, 0.0, 0.0, 0.09350364947160915, 0.0, 0.14668074597899614, 0.060670322800377645, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "\n",
    "sklearn_representation2 = sklearn_tfidf.fit_transform(all_no_stopwords)\n",
    "print(sklearn_representation2.toarray()[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.131672425261\n",
      "0.0806557853596\n",
      "0.0689436022552\n",
      "0.0688036584055\n",
      "0.112424613116\n",
      "0.138875377699\n",
      "0.0649261995438\n",
      "0.106110345141\n",
      "0.0908766456274\n",
      "0.131672425261\n",
      "1.0\n",
      "0.0894817570443\n",
      "0.0852865529508\n",
      "0.0921987451702\n",
      "0.107455594707\n",
      "0.100011850508\n",
      "0.056724992923\n",
      "0.075359604726\n",
      "0.0441342979148\n",
      "0.0806557853596\n",
      "0.0894817570443\n",
      "1.0\n",
      "0.0629504419443\n",
      "0.173518280823\n",
      "0.12546847657\n",
      "0.109766965062\n",
      "0.147535633488\n",
      "0.0715585467905\n",
      "0.0628736680971\n",
      "0.0689436022552\n",
      "0.0852865529508\n",
      "0.0629504419443\n",
      "1.0\n",
      "0.0948132468857\n",
      "0.107558421961\n",
      "0.0725501119808\n",
      "0.0987596203735\n",
      "0.0712499074161\n",
      "0.0822234552128\n",
      "0.0688036584055\n",
      "0.0921987451702\n",
      "0.173518280823\n",
      "0.0948132468857\n",
      "1.0\n",
      "0.0967759530441\n",
      "0.0359956130701\n",
      "0.0677823357003\n",
      "0.0583960326789\n",
      "0.0911724101465\n",
      "0.112424613116\n",
      "0.107455594707\n",
      "0.12546847657\n",
      "0.107558421961\n",
      "0.0967759530441\n",
      "1.0\n",
      "0.178640802861\n",
      "0.154680232218\n",
      "0.0795421018847\n",
      "0.134061614177\n",
      "0.138875377699\n",
      "0.100011850508\n",
      "0.109766965062\n",
      "0.0725501119808\n",
      "0.0359956130701\n",
      "0.178640802861\n",
      "1.0\n",
      "0.0525163744302\n",
      "0.104588516046\n",
      "0.0907679241918\n",
      "0.0649261995438\n",
      "0.056724992923\n",
      "0.147535633488\n",
      "0.0987596203735\n",
      "0.0677823357003\n",
      "0.154680232218\n",
      "0.0525163744302\n",
      "1.0\n",
      "0.0478670966149\n",
      "0.0693728229195\n",
      "0.106110345141\n",
      "0.075359604726\n",
      "0.0715585467905\n",
      "0.0712499074161\n",
      "0.0583960326789\n",
      "0.0795421018847\n",
      "0.104588516046\n",
      "0.0478670966149\n",
      "1.0\n",
      "0.0582182901923\n",
      "0.0908766456274\n",
      "0.0441342979148\n",
      "0.0628736680971\n",
      "0.0822234552128\n",
      "0.0911724101465\n",
      "0.134061614177\n",
      "0.0907679241918\n",
      "0.0693728229195\n",
      "0.0582182901923\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for doc1 in range(0,10):\n",
    "    for doc2 in range(0,10):\n",
    "        print (cosine_similarity(sklearn_representation2.toarray()[doc1],sklearn_representation2.toarray()[doc2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment: comparing the two similarities with and without stopwords, the similarity changed quite a lot. The highest similarities before with stopwords was only 0.12, and the without stopwords the highest is 0.17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BracketParseCorpusReader in 'C:\\\\Users\\\\YY\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\treebank\\\\combined'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "print (treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist, ConditionalFreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cfd = ConditionalFreqDist()\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag.lower() in ['nn','nns','nn$','nn-tl','nn+bez', 'nn+hvz', \n",
    "                           'nns$','np','np$','np+bez','nps', 'nps$','nr',\n",
    "                           'np-tl','nrs','nr$']\n",
    "\n",
    "for sentence in treebank.tagged_sents():\n",
    "    for (index, tagtuple) in enumerate(sentence):\n",
    "        (token, tag) = tagtuple\n",
    "        token = token.lower()\n",
    "        if token not in stopwords_list and is_noun(tag):\n",
    "            window = sentence[index+1:index+5]\n",
    "            for (window_token, window_tag) in window:\n",
    "                window_token = window_token.lower()\n",
    "                if window_token not in stopwords_list and is_noun(window_tag):\n",
    "                    cfd[token][window_token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chief\n"
     ]
    }
   ],
   "source": [
    "print(cfd['chairman'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%\n"
     ]
    }
   ],
   "source": [
    "print(cfd['profit'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chief\n"
     ]
    }
   ],
   "source": [
    "print(cfd['president'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonds\n"
     ]
    }
   ],
   "source": [
    "print(cfd['investors'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streets\n"
     ]
    }
   ],
   "source": [
    "print(cfd['city'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "officer\n"
     ]
    }
   ],
   "source": [
    "print(cfd['chief'].max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
